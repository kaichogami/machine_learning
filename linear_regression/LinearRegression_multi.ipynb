{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression with multiple feature vector\n",
    "\n",
    "Continuing from that last notebook with single feature [LinearRegression](https://github.com/kaichogami/machine_learning/blob/master/linear_regression/single_variable.py)(if you haven't read it, go through it!), we will now implement a LinearRegression class which will handle multiple feature vector. We will also solve it through analytical method. Generally analytical method is faster when `n < 10000`, `n` being the number of features, but gets slow and takes a lot of memory if `n` gets greater. Gradiant descent is preferred in such case. Since our code doesn't implement parallelization, our code won't be practical in large data set. Finally we will compare our `true y` and `predicted y` using a scoring method.\n",
    "\n",
    "### Theory\n",
    "In the last notebook we worked around with a single feature vector and drew a plot of straight line. Our equation was\n",
    "$$y = w_0 + w_1X$$\n",
    "`w1` is our coefficient and `w0` is the intercept. Extending the same idea, where our y is a linear combination of many variables(features) we can re-write our equation as\n",
    "$$y = w_0 + w_1X_1 + w_2X_2 + .... + w_nX_n$$\n",
    "where `X1`... `Xn` is our features and `w0`...`wn` is our weights. We will apply gradiant descent algorithm to optimise the same cost function.\n",
    "$$J(\\theta_0, \\theta_1, ...) = \\sum_{i=1}^n{(y_i - \\theta_0 + \\theta_1X_1^{(i)} + \\theta_2X_2^{(i)}+ ..... +\\theta_mX_m^{(i)})^2}$$\n",
    "\n",
    "For simplicity\n",
    "$$J(\\theta_0) = \\sum_{i=1}^n{(y_i - \\theta_0 + \\theta_1X_1^{(i)} + \\theta_2X_2^{(i)}+.....+\\theta_mX_m^{(i)})^2}$$\n",
    "\n",
    "#### Matrix representation\n",
    "Before moving on, its important to understand the notation of matrices. \n",
    "$$X_1^{(i)}$$\n",
    "Lets look at the above variable. Capital letter indicates its a vector or a matrix. The subscript 1 indicates its the first feature in our matrix. Features are usually the columns and the rows are the examples. Let\n",
    "$$A \\in \\mathbb{R}^{2*3}$$\n",
    "And the matrix be\n",
    "$$\\begin{bmatrix}\n",
    "1 && 2 && 3 \\\\\n",
    "4 && 5 && 6\\end{bmatrix}$$\n",
    "This matrix has a dimension of 2 * 3, 2 rows and 3 columns. Rows are the instances of the data and columns are the features. So `A` has 3 features. Our equation above will then become  \n",
    "$$y = w_0 + w_1X_1 + w_2X_2 + w_3X_3$$\n",
    "\n",
    "Also\n",
    "\n",
    "$$X_1^{(i)}$$\n",
    "Represents the ith example of the first feature. Watch [this](https://youtu.be/j-MP6CDJiJM?list=PLnnr1O8OWc6asSH0wOMn5JjgSlqNiK2C4) Andrew's video for more clear understanding.\n",
    "\n",
    "#### Optmisation\n",
    "Similar to our last notebook, we will use gradiant descent to minimise our cost function. We will find our weights by differentating each wrt each weight, and using the slope in our algorithm. Our equation earlier(single feature) was\n",
    "$$\\frac{\\delta}{\\delta\\theta_1} = \\frac{2 \\sum_{i=1}^n{((\\theta_0 + \\theta_1X_i)}X_i - y_i)} {N}$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\frac{\\delta}{\\delta\\theta_0} = \\frac{2 \\sum_{i=1}^n{(\\theta_0 + \\theta_1X_i} - y_i)} {N}$$\n",
    "\n",
    "Where `X_i` was the ith example of the first feature. Extending the same idea, we can find weight the `j_th` weight with the equation\n",
    "\n",
    "$$\\theta_j = \\theta_j - \\alpha \\frac{2}{m} \\sum_{i=1}^m{(h_\\theta(X^{(i)}) - y^{(i)})}X_j^{(i)}$$\n",
    "\n",
    "#### Analytical\n",
    "Instead of all the hassle to use the cost function and optimising it, we can simply find the weights by equating the slope of the derivative to the zero. This requires no choosing of learning rate, and no need of deciding the stopping criteria. We will not derive the equations here as they are beyond the scope of this notebook. We will use the derived matrix equation to find the weights.\n",
    "\n",
    "$$\\theta = {(X^TX)^{-1})X^Ty}$$\n",
    "\n",
    "Notice that we require to find the inverse of a matrix, which has a time complexity of `O(n^3)`. This is the biggest disadvantage of this method. When the features are very very large this becomes extremely slow, in which case we use gradiant descent. I haven't tested out it out fully yet, but with 1000 features this works pretty well. I would choose this method over gradiant descent if say n < 10000 (I am just repeating what Andrew said in his video here).\n",
    "\n",
    "Now that both methods are clear, lets move on with the coding part. The code is hosted [here](https://github.com/kaichogami/machine_learning/blob/master/linear_regression/multi_variables.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for gradiant to fit is 19.4937679768 seconds\n",
      "Time taken for analytic method to fit is 0.0166280269623 seconds\n",
      "Score for gradiant method is 0.197290902174\n",
      "Score for analytical method is 0.200896572395\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from multi_variables import MultiLinearRegression\n",
    "\n",
    "X = np.random.randint(1, 100, [1000, 200]).astype(np.float)\n",
    "y  = [sum(X[i]) * float(np.random.random_integers(1, 1000, 1)) + 1000\n",
    "                    for i in xrange(1000)]\n",
    "sc = StandardScaler().fit(X)\n",
    "X = sc.transform(X)\n",
    "\n",
    "mlr = MultiLinearRegression(algorithm='gradiant', alpha=0.001, max_iter=300)\n",
    "start_gradiant = time.time()\n",
    "mlr.fit(X, y)\n",
    "stop_gradiant = time.time()\n",
    "y_bar_gradiant = mlr.transform(X)\n",
    "\n",
    "mlr = MultiLinearRegression(algorithm='analytic')\n",
    "start_analytic = time.time()\n",
    "mlr.fit(X, y)\n",
    "stop_analytic = time.time()\n",
    "y_bar_analytic = mlr.transform(X)\n",
    "\n",
    "print(\"Time taken for gradiant to fit is {0} seconds\").format(stop_gradiant - start_gradiant)\n",
    "print(\"Time taken for analytic method to fit is {0} seconds\").format(stop_analytic - start_analytic)\n",
    "\n",
    "# Calculate the score of our prediction.\n",
    "def score(y, y_bar):\n",
    "    return 1 - ((y-y_bar).var() / np.var(y))\n",
    "\n",
    "print(\"Score for gradiant method is {0}\").format(score(y, y_bar_gradiant))\n",
    "print(\"Score for analytical method is {0}\").format(score(y, y_bar_analytic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lot has happened in the above code. We first import the relevant packges. `StandardScaler` is a scikit-learn package, which standarizes the code by removing the mean. [read here](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for more details. We then generate our input variable X with a length of 1000 and 200 features. `y` is the label for our data, which is in some way related to X. Its a very bad relation, forgive me for that. The rest is self explainatory, we fit our data first with our model and transform our input matrix to `y` and save it `y_bar`. \n",
    "Finally we evaluate our score or how well our model performs. Higher the value, more better it is. The scoring equation is\n",
    "\n",
    "$$score = 1 - \\frac{Var(y - \\bar{y})}{Var(y)}$$\n",
    "\n",
    "Also note the time taken to fit the data by both the algorithms. `Gradiant descent` algorithm seems to be performing very badly. This is because we have not optimised the algorithm(parallelism, numpy functions) in the code.\n",
    "This ends the notebook with a brief introduction to multiple feature LinearRegression. Thank you for reading!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
